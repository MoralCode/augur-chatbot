# docker-compose.yml
# This file defines the multi-container application setup.
# All Python services now build from the single Dockerfile in the root directory.

version: '3.8'

services:
  # Ollama Service: Runs the core Ollama model server.
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ~/.ollama:/root/.ollama:Z
    restart: unless-stopped
    networks:
      - ollama_net
    # Healthcheck to ensure the Ollama server is running before other services try to use it.
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434"]
      interval: 5s
      timeout: 3s
      retries: 5

  # New Service to pull and load the model on startup.
  ollama_model_loader:
    image: ollama/ollama
    container_name: ollama_model_loader
    # This command runs after the ollama service is healthy.
    # It pulls the model (if not present) and keeps it in memory.
    command: ollama run llama3.2:3b-instruct-fp16 --keepalive 160m
    depends_on:
      ollama:
        condition: service_healthy # Waits for the healthcheck to pass.
    volumes:
      - ~/.ollama:/root/.ollama:Z # Shares the same volume to access/store models.
    networks:
      - ollama_net
    restart: unless-stopped

  # LlamaStack Service: Provides the distribution layer.
  llamastack:
    image: llamastack/distribution-ollama:0.2.9
    container_name: llamastack
    ports:
      - "8321:8321"
    volumes:
      - ~/.llama:/root/.llama:Z
    environment:
      - INFERENCE_MODEL=llama3.2:3b-instruct-fp16
      - OLLAMA_URL=http://ollama:11434
    # Now depends on the model loader, which in turn depends on ollama.
    depends_on:
      - ollama_model_loader
    restart: unless-stopped
    networks:
      - ollama_net
     # Healthcheck to ensure the LlamaStack service is ready before other services start.
    healthcheck:
      # This check attempts to hit a health endpoint. You may need to adjust the path
      # (e.g., to /v1/health) depending on the LlamaStack distribution's API.
      test: ["CMD", "curl", "-f", "http://localhost:8321/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 20s # Gives the container time to initialize before the first check.


  # MCP Registration Service: Builds from the root Dockerfile and runs the registration script.
  register_mcp:
    build: .
    container_name: register_mcp
    command: python /app/register_mcp.py
    depends_on:
      llamastack:
        condition: service_healthy
    restart: on-failure
    networks:
      - ollama_net

  # MCP Execution Service: Builds from the root Dockerfile and runs the Uvicorn server.
  mcp_execute:
    build: .
    container_name: mcp_execute
    command: uvicorn mcp_execute:app --host 0.0.0.0 --port 9002 --app-dir /app
    ports:
      - "9002:9002"
    depends_on:
      llamastack:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ollama_net

  # Streamlit UI Service: Builds from the root Dockerfile and runs the Streamlit app.
  streamlit_ui:
    build: .
    container_name: streamlit_ui
    command: streamlit run /app/ui.py --server.port=8501 --server.address=0.0.0.0
    ports:
      - "8501:8501"
    environment:
      - BASE_URL=http://llamastack:8321
    depends_on:
      llamastack:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ollama_net

# Network to allow services to communicate with each other by name.
networks:
  ollama_net:
    driver: bridge
